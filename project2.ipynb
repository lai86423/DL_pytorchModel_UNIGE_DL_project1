{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zT1x1BON2Ohg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gMVNnkv08OXG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Generates a training and a test set of 1, 000 points sampled uniformly in [0, 1] , \n",
        "each with a label 0 if outside the disk centered at (0.5, 0.5) of radius 1/ 2π, and 1 inside\n",
        "\"\"\"\n",
        "def generate_disc_set(nb): \n",
        "    input = torch.rand(nb, 2, dtype=torch.float32)\n",
        "    label = torch.zeros(nb, dtype=torch.int64)\n",
        "    # input = torch.empty((1000, 2)).normal(mean=0,var)\n",
        "    dis = 1 / (2 * math.pi)\n",
        "    for i in range(input.size(0)):\n",
        "      if ((input[i] - 0.5).square().sum()) < dis:\n",
        "        # print(i)\n",
        "        label[i] = 1\n",
        "      # print(input[i].square().sum())\n",
        "    # print(label)\n",
        "    return input, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vYfnCZQSkMSV"
      },
      "outputs": [],
      "source": [
        "def sigmoid(result):\n",
        "    for idx,layer in enumerate(result):\n",
        "        result[idx] = 1 / (1 + math.exp(-layer))   \n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D4dXlRBQX0C6"
      },
      "outputs": [],
      "source": [
        "def sigma(delta, weight, result): # Calculation of hidden layers (Derived from the relationship between the front and back layers)\n",
        "    sig = torch.zeros(1, weight.size(0) - 1) #except the output layer\n",
        "    # print('sig_size:', sig.size())\n",
        "    for neu in range(weight.size(0) - 1):  # Calculate the delta of each neuron one by one 神經元逐一計算相對的delta\n",
        "        for x in range(weight.size(1)): #for RELU, if result>=0, [sigma = delta*weight] 利用weight的後一位判斷應該要用的維度，判斷原先的輸出是否大於零\n",
        "            if result[0][x] > 0: \n",
        "                sig[0][neu] += (delta[0][x] * weight[neu][x])\n",
        "    return sig\n",
        "    # print(weight)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrS5Hw8A77oO"
      },
      "outputs": [],
      "source": [
        "def loss(v, t):\n",
        "    # print((v - t).pow(2).sum())\n",
        "    return (v - t).pow(2).sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEtUJ_e283DL",
        "outputId": "4ed03622-48f5-4330-aca0-8a5149600b35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input: tensor([[0.6632, 0.7177],\n",
            "        [0.6711, 0.2280],\n",
            "        [0.5321, 0.3640],\n",
            "        ...,\n",
            "        [0.4434, 0.6406],\n",
            "        [0.6889, 0.1887],\n",
            "        [0.2362, 0.6555]])  label: tensor([1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
            "        1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
            "        1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
            "        1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
            "        0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1,\n",
            "        1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
            "        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
            "        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
            "        1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
            "        0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
            "        0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
            "        0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
            "        0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
            "        1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
            "        0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,\n",
            "        1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
            "        0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
            "        0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
            "        0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
            "        0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
            "        1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
            "        1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
            "        1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
            "        0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,\n",
            "        0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
            "        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1,\n",
            "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
            "        1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1,\n",
            "        0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
            "        0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
            "        1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
            "        1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
            "        1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
            "        0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
            "        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
            "        0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
            "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
            "        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
            "        0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1])\n",
            "epoch: 0\n",
            "acc_loss: tensor(295.5884)\n",
            "acc: 0.614\n",
            "------\n",
            "epoch: 1\n",
            "acc_loss: tensor(195.8992)\n",
            "acc: 0.702\n",
            "------\n",
            "epoch: 2\n",
            "acc_loss: tensor(162.9507)\n",
            "acc: 0.775\n",
            "------\n",
            "epoch: 3\n",
            "acc_loss: tensor(139.7721)\n",
            "acc: 0.837\n",
            "------\n",
            "epoch: 4\n",
            "acc_loss: tensor(124.6133)\n",
            "acc: 0.887\n",
            "------\n",
            "epoch: 5\n",
            "acc_loss: tensor(113.5334)\n",
            "acc: 0.917\n",
            "------\n",
            "epoch: 6\n",
            "acc_loss: tensor(105.2692)\n",
            "acc: 0.939\n",
            "------\n",
            "epoch: 7\n",
            "acc_loss: tensor(98.8978)\n",
            "acc: 0.946\n",
            "------\n",
            "epoch: 8\n",
            "acc_loss: tensor(93.4535)\n",
            "acc: 0.943\n",
            "------\n",
            "epoch: 9\n",
            "acc_loss: tensor(89.1927)\n",
            "acc: 0.941\n",
            "------\n",
            "epoch: 10\n",
            "acc_loss: tensor(85.9575)\n",
            "acc: 0.933\n",
            "------\n",
            "epoch: 11\n",
            "acc_loss: tensor(82.9465)\n",
            "acc: 0.934\n",
            "------\n",
            "epoch: 12\n",
            "acc_loss: tensor(80.5173)\n",
            "acc: 0.933\n",
            "------\n",
            "epoch: 13\n",
            "acc_loss: tensor(78.2826)\n",
            "acc: 0.933\n",
            "------\n",
            "epoch: 14\n",
            "acc_loss: tensor(75.2729)\n",
            "acc: 0.928\n",
            "------\n",
            "epoch: 15\n",
            "acc_loss: tensor(72.4887)\n",
            "acc: 0.931\n",
            "------\n",
            "epoch: 16\n",
            "acc_loss: tensor(69.4948)\n",
            "acc: 0.935\n",
            "------\n",
            "epoch: 17\n",
            "acc_loss: tensor(67.0184)\n",
            "acc: 0.938\n",
            "------\n",
            "epoch: 18\n",
            "acc_loss: tensor(65.0424)\n",
            "acc: 0.934\n",
            "------\n",
            "epoch: 19\n",
            "acc_loss: tensor(63.3353)\n",
            "acc: 0.934\n",
            "------\n",
            "epoch: 20\n",
            "acc_loss: tensor(61.9142)\n",
            "acc: 0.933\n",
            "------\n",
            "epoch: 21\n",
            "acc_loss: tensor(60.3282)\n",
            "acc: 0.934\n",
            "------\n",
            "epoch: 22\n",
            "acc_loss: tensor(58.5557)\n",
            "acc: 0.935\n",
            "------\n",
            "epoch: 23\n",
            "acc_loss: tensor(57.6555)\n",
            "acc: 0.936\n",
            "------\n",
            "epoch: 24\n",
            "acc_loss: tensor(56.5321)\n",
            "acc: 0.934\n",
            "------\n",
            "epoch: 25\n",
            "acc_loss: tensor(55.6966)\n",
            "acc: 0.936\n",
            "------\n",
            "epoch: 26\n",
            "acc_loss: tensor(54.9872)\n",
            "acc: 0.941\n",
            "------\n",
            "epoch: 27\n",
            "acc_loss: tensor(54.5930)\n",
            "acc: 0.942\n",
            "------\n",
            "epoch: 28\n",
            "acc_loss: tensor(54.3756)\n",
            "acc: 0.944\n",
            "------\n",
            "epoch: 29\n",
            "acc_loss: tensor(53.8061)\n",
            "acc: 0.946\n",
            "------\n",
            "epoch: 30\n",
            "acc_loss: tensor(52.8979)\n",
            "acc: 0.938\n",
            "------\n",
            "epoch: 31\n",
            "acc_loss: tensor(51.9709)\n",
            "acc: 0.933\n",
            "------\n",
            "epoch: 32\n",
            "acc_loss: tensor(51.1013)\n",
            "acc: 0.935\n",
            "------\n",
            "epoch: 33\n",
            "acc_loss: tensor(50.3450)\n",
            "acc: 0.935\n",
            "------\n",
            "epoch: 34\n",
            "acc_loss: tensor(49.3782)\n",
            "acc: 0.936\n",
            "------\n",
            "epoch: 35\n",
            "acc_loss: tensor(48.5471)\n",
            "acc: 0.938\n",
            "------\n",
            "epoch: 36\n",
            "acc_loss: tensor(47.8426)\n",
            "acc: 0.942\n",
            "------\n",
            "epoch: 37\n",
            "acc_loss: tensor(47.3456)\n",
            "acc: 0.942\n",
            "------\n",
            "epoch: 38\n",
            "acc_loss: tensor(47.0518)\n",
            "acc: 0.944\n",
            "------\n",
            "epoch: 39\n",
            "acc_loss: tensor(46.8771)\n",
            "acc: 0.95\n",
            "------\n",
            "epoch: 40\n",
            "acc_loss: tensor(46.4745)\n",
            "acc: 0.952\n",
            "------\n",
            "epoch: 41\n",
            "acc_loss: tensor(46.1063)\n",
            "acc: 0.954\n",
            "------\n",
            "epoch: 42\n",
            "acc_loss: tensor(45.1824)\n",
            "acc: 0.95\n",
            "------\n",
            "epoch: 43\n",
            "acc_loss: tensor(44.3809)\n",
            "acc: 0.95\n",
            "------\n",
            "epoch: 44\n",
            "acc_loss: tensor(43.6957)\n",
            "acc: 0.949\n",
            "------\n",
            "epoch: 45\n",
            "acc_loss: tensor(42.7926)\n",
            "acc: 0.95\n",
            "------\n",
            "epoch: 46\n",
            "acc_loss: tensor(42.1370)\n",
            "acc: 0.949\n",
            "------\n",
            "epoch: 47\n",
            "acc_loss: tensor(41.6391)\n",
            "acc: 0.949\n",
            "------\n",
            "epoch: 48\n",
            "acc_loss: tensor(41.0824)\n",
            "acc: 0.948\n",
            "------\n",
            "epoch: 49\n",
            "acc_loss: tensor(40.5369)\n",
            "acc: 0.945\n",
            "------\n"
          ]
        }
      ],
      "source": [
        "input, label = generate_disc_set(1000)\n",
        "test_input, test_label = generate_disc_set(1000)\n",
        "\n",
        "print('input:', input, ' label:', label)\n",
        "\n",
        "class Module:\n",
        "    def __init__(self, layer = 3, neuron = 25):\n",
        "        self.layer = layer\n",
        "        self.neu = neuron\n",
        "        self.lr = 0.01\n",
        "        self.input_shape = 2\n",
        "        self.output_shape = 1    \n",
        "\n",
        "    def init_weight(self):\n",
        "        self.weight = []\n",
        "        ##for input layer\n",
        "        self.weight.append(torch.rand(self.input_shape, self.neu) * 2 - 1) #let random number range 0~1 => -1~1\n",
        "        ##for hidden layer\n",
        "        for i in range(1, self.layer):\n",
        "            self.weight.append(torch.rand(self.neu+1, self.neu) * 2 - 1) #input dimension = neu number + 1(bias)\n",
        "        ##for ouput layer\n",
        "        self.weight.append(torch.rand(self.neu+1, self.output_shape) * 2 - 1)\n",
        "        # print('weight:', self.weight)\n",
        "        # print('weight size:', self.weight[1].size())\n",
        "\n",
        "    def forward_pass(self, train_data):\n",
        "        result = []\n",
        "        ## input layer\n",
        "        result.append(torch.mm(train_data.expand(1, -1), self.weight[0]).relu())\n",
        "        result[0] = torch.cat((result[0], torch.tensor([[1]])), 1) #caculate the bias by adding 1 to the first posotion of the result 補1\n",
        "        ## hidden layers\n",
        "        for idx in range(1, self.layer): \n",
        "            result.append(torch.mm(result[idx - 1], self.weight[idx]).relu())\n",
        "            result[idx] = torch.cat((result[idx], torch.tensor([[1]])), 1)\n",
        "        ## output layer\n",
        "        result.append(sigmoid(torch.mm(result[self.layer-1], self.weight[self.layer]))) #last layer go through sigmoid function\n",
        "\n",
        "        # print('result:', result)\n",
        "        return result\n",
        "\n",
        "    def backward_pass(self, result, target):\n",
        "        delta = []\n",
        "        ## output layer \n",
        "        delta_temp = (target - result[self.layer]).mul(result[self.layer].mul(1 - result[self.layer]))\n",
        "        delta.append(delta_temp)\n",
        "\n",
        "        for k in range(self.layer-1, -1, -1):\n",
        "            # print('back_lay:', k)\n",
        "            # print('delta_0:', delta[0])\n",
        "            # print('weight_size:', self.weight[k + 1].size())\n",
        "            \n",
        "            delta_temp = sigma(delta[0], self.weight[k + 1], result[k + 1])\n",
        "            delta.insert(0, delta_temp) #往前插到第一個位置\n",
        "            # print('-----')\n",
        "        return delta\n",
        "\n",
        "    def modi_weight(self, delta, input, result):\n",
        "        # print('weight_a:', self.weight[0])\n",
        "        # print('input:', input)\n",
        "        # print('delta:', delta[0])\n",
        "        self.weight[0] += (self.lr * (input.reshape(2, 1).mm(delta[0])))\n",
        "        for i in range(1, self.layer + 1):\n",
        "            # print('i:', i)\n",
        "            # print(delta[i - 1])\n",
        "            self.weight[i] += (self.lr * (result[i - 1].t().mm(delta[i])))\n",
        "        # print(self.weight)\n",
        "        # print('modi_weight:', self.weight)\n",
        "\n",
        "    def cal_acc(self, data, target):\n",
        "        right = 0\n",
        "        for i in range(data.size(0)):\n",
        "            output = self.forward_pass(data[i])\n",
        "            if output[-1].round() == target[i]:\n",
        "              right += 1\n",
        "        return right\n",
        "            \n",
        "\n",
        "\n",
        "epoch = 50\n",
        "\n",
        "x1 = Module(layer=3, neuron=25)\n",
        "x1.init_weight()\n",
        "\n",
        "for r in range(epoch):\n",
        "    acc_loss = 0\n",
        "    print('epoch:', r)\n",
        "    for i in range(len(input)):\n",
        "        result = x1.forward_pass(input[i])\n",
        "        acc_loss += loss(label[i], result[-1])\n",
        "        delta = x1.backward_pass(result, label[i])\n",
        "        x1.modi_weight(delta, input[i], result)\n",
        "    print('acc_loss:', acc_loss)\n",
        "    right = x1.cal_acc(test_input, test_label)\n",
        "    print('acc:', right/test_input.size(0))\n",
        "    print('------')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "project2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}